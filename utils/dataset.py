import os, json, random
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from tqdm import tqdm
import faiss


class ImageTextPairDataset(Dataset):
    def __init__(self, image_dir, caption_json, is_train=True, 
                 tokenizer_name="bert-base-uncased", 
                 neg_strategy="random", max_samples=None):
        self.image_dir = image_dir
        self.is_train = is_train
        self.neg_strategy = neg_strategy
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

        with open(caption_json, 'r') as f:
            all_data = json.load(f)
        self.data = all_data[:max_samples] if max_samples else all_data
        print(f"ğŸ“¦ åŠ è½½å›¾æ–‡å¯¹: {len(self.data)} æ¡")

        if self.neg_strategy == "hard" and self.is_train:
            self._prepare_hard_negatives()

    def _prepare_hard_negatives(self):
        print("ğŸ§  æ„é€  Hard Negative æ ·æœ¬ï¼ˆFAISSåŠ é€Ÿï¼‰...")
        model = BertModel.from_pretrained("bert-base-uncased").eval().cuda()
        tokenizer = self.tokenizer
        text_features = []

        for item in tqdm(self.data, desc="ç¼–ç æ–‡æœ¬"):
            inputs = tokenizer(item["caption"], return_tensors="pt", truncation=True, max_length=40, padding="max_length").to("cuda")
            with torch.no_grad():
                emb = model(**inputs).last_hidden_state[:, 0, :].cpu().numpy()
            text_features.append(emb[0])

        text_features = np.stack(text_features).astype("float32")
        norms = np.linalg.norm(text_features, axis=1, keepdims=True)
        text_features = text_features / norms  # normalize for cosine

        import faiss
        dim = text_features.shape[1]
        index = faiss.IndexFlatIP(dim)  # cosine similarity by inner product
        index.add(text_features)

        D, I = index.search(text_features, 10)  # æŸ¥æ‰¾ top10 ç›¸ä¼¼é¡¹ï¼ˆå«è‡ªèº«ï¼‰

        self.neg_index_map = {}
        for i in range(len(self.data)):
            for j in I[i][1:]:  # è·³è¿‡è‡ªèº«
                if self.data[j]["image"] != self.data[i]["image"]:
                    self.neg_index_map[i] = j
                    break
        print(f"âœ… å®Œæˆ Hard Negative æ„é€ ï¼Œè¦†ç›–æ ·æœ¬æ•°: {len(self.neg_index_map)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        img_path = os.path.join(self.image_dir, sample["image"])

        try:
            image = Image.open(img_path).convert("RGB")
            image = self.transform(image)
        except:
            image = torch.zeros(3, 224, 224)

        if self.is_train and random.random() < 0.5:
            # è´Ÿæ ·æœ¬
            if self.neg_strategy == "hard" and hasattr(self, "neg_index_map"):
                neg_idx = self.neg_index_map.get(idx, random.randint(0, len(self.data)-1))
                text = self.data[neg_idx]["caption"]
                label = 0
            else:
                # fallback to random
                other = random.choice(self.data)
                while other["image"] == sample["image"]:
                    other = random.choice(self.data)
                text = other["caption"]
                label = 0
        else:
            # æ­£æ ·æœ¬
            text = sample["caption"]
            label = 1

        encoded = self.tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=40)
        return {
            "image": image,
            "input_ids": encoded["input_ids"].squeeze(),
            "attention_mask": encoded["attention_mask"].squeeze(),
            "label": label,
            "image_name": sample["image"]
        }
